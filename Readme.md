# Zillow ETL Pipeline with Apache Airflow

## ğŸ“Œ Project Overview

This project implements an **End-to-End ETL Pipeline** to extract real estate data from Zillow using **RapidAPI**, transform the data, and load it into **Amazon Redshift**. The workflow is orchestrated using **Apache Airflow** running on an **EC2 instance** to automate data ingestion and processing. The pipeline also integrates **AWS S3** for intermediate storage and **AWS QuickSight** for visualization.

## ğŸ—ï¸ Tech Stack

- **Python** â€“ Data extraction, transformation, and processing
- **Apache Airflow (on EC2)** â€“ Workflow orchestration
- **RapidAPI** â€“ Zillow API integration
- **Pandas & NumPy** â€“ Data manipulation and transformation
- **Amazon Redshift** â€“ Data warehouse storage
- **AWS S3** â€“ Intermediate storage and backup
- **AWS Lambda** â€“ Automated processing and transformation
- **AWS QuickSight** â€“ Data visualization

## ğŸ“Š ETL Pipeline Workflow

1. **Extract**: Retrieve Zillow real estate data from RapidAPI using a Python script.
2. **Transform**: Convert JSON response to CSV format using **AWS Lambda** and Pandas.
3. **Load**: Store the processed CSV data in **Amazon S3** and then transfer it to **Amazon Redshift**.
4. **Automate**: Use **Apache Airflow (running on EC2)** to schedule and monitor the ETL jobs.
5. **Visualize**: Connect **Amazon Redshift** to **AWS QuickSight** for interactive data visualization and analysis.

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/zillow-etl-airflow.git
cd zillow-etl-airflow
```

### 2ï¸âƒ£ Setup Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Apache Airflow on AWS EC2
```bash
# Update packages and install dependencies
sudo apt update && sudo apt install -y python3-pip python3-venv

# Create a virtual environment and activate it
python3 -m venv airflow_venv
source airflow_venv/bin/activate

# Install Apache Airflow
pip install apache-airflow

# Initialize Airflow
airflow db init
airflow users create --username admin --password admin --firstname John --lastname Doe --role Admin --email admin@example.com

# Start Airflow web server and scheduler
airflow webserver --port 8080 &
airflow scheduler &

```

### 5ï¸âƒ£Run the ETL Pipeline

- **Trigger DAG in Airflow UI**: Navigate to [`http://<EC2-PUBLIC-IP>:8080`](http://<EC2-PUBLIC-IP>:8080) and trigger the DAG manually.
- **Check Logs**: Monitor execution logs in Airflow UI.



## ğŸ“‚ Project Structure

zillow-etl-airflow â”œâ”€â”€ dags â”‚ â”œâ”€â”€ zillow_analytics.py # Airflow DAG for ETL workflow â”‚ â”œâ”€â”€ utils.py # Helper functions for data processing â”œâ”€â”€ scripts â”‚ â”œâ”€â”€ Zillow_copyRawJsonFile_LF.py # Lambda function to copy raw JSON from S3 â”‚ â”œâ”€â”€ Zillow_transform_convert_2_csv_LF.py # Lambda function to transform JSON to CSV â”‚ â”œâ”€â”€ load.py # Data loading script â”œâ”€â”€ config â”‚ â”œâ”€â”€ config_api.json # API configuration file â”œâ”€â”€ requirements.txt # Python dependencies â”œâ”€â”€ .env # Environment variables (ignored in .gitignore) â”œâ”€â”€ README.md # Project documentation